{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import findspark\n",
    "findspark.init()\n",
    "# Initate spark configuration and spark context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "#from pyspark.context import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, size, split, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DecimalType\n",
    "#from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.install_pypi_package(\"pandas\")\n",
    "#sc.install_pypi_package(\"findspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64950d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_aws_ver = '3.3.2'\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"apollo-sena\") \\\n",
    "    .config(\"spark.jars.packages\", f\"org.apache.hadoop:hadoop-aws:{hadoop_aws_ver},org.apache.hadoop:hadoop-common:{hadoop_aws_ver},org.apache.hadoop:hadoop-client:{hadoop_aws_ver}\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.DefaultAWSCredentialsProviderChain') \\\n",
    "    .config('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true') \\\n",
    "    .config('spark.executor.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true') \\\n",
    "    .config('spark.driver.memory', '8G') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c537ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca624eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3://apollo-mission/apollo-simge-test/CommitFile.parquet\") #filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = df.filter(df.language == \"Python\")\n",
    "\n",
    "rdd_cursor = cursor.rdd.flatMap(lambda x : str(x[3].encode(\"utf-8\")).split(\"\\n\"))\n",
    "schemaDT = spark.createDataFrame(rdd_cursor, StringType(), True).cache()\n",
    "schemaDT.createOrReplaceTempView(\"rdd_cursor\")\n",
    "sqlDF1 = spark.sql(\"SELECT trim(substr(value,7,instr(value,'as')-7)) as library FROM rdd_cursor where trim(value) like 'import%as%' \" )\n",
    "sqlDF2 = spark.sql(\"SELECT trim(substr(value,7)) as library FROM rdd_cursor where trim(value) like 'import%' and trim(value)  not like '% as %' \" )\n",
    "sqlDF3 = spark.sql(\"SELECT trim(substr(value,5,instr(value,'import')-6)) as library FROM rdd_cursor where trim(value) like 'from%' and  trim(substr(value,5,instr(value,'import')-6)) not like '.%' and trim(substr(value,5,instr(value,'import')-6)) not like '' \")\n",
    "DF =sqlDF1.union(sqlDF2)\n",
    "DF_final = DF.union(sqlDF3)\n",
    "\n",
    "DF_libcnt = DF_final.groupby(\"library\").count().orderBy(\"count\", ascending = False)\n",
    "\n",
    "DF_libcnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = df.filter(df.language == \"Java\")\n",
    "\n",
    "rdd_cursor = cursor.rdd.flatMap(lambda x : str(x[3].encode(\"utf-8\")).split(\"\\n\"))\n",
    "schemaDT = spark.createDataFrame(rdd_cursor, StringType(), True).cache()\n",
    "schemaDT.createOrReplaceTempView(\"rdd_cursor\")\n",
    "sqlDF = spark.sql(\"SELECT replace(substr(trim(value),7),';','') as library FROM rdd_cursor where trim(value) like 'import%;'  \" )\n",
    "\n",
    "DF_final = sqlDF\n",
    "\n",
    "\n",
    "DF_libcnt = DF_final.groupby(\"library\").count().orderBy(\"count\", ascending = False)\n",
    "\n",
    "DF_libcnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = df.filter(df.language == \"JavaScript\")\n",
    "\n",
    "rdd_cursor = cursor.rdd.flatMap(lambda x : str(x[3].encode(\"utf-8\")).split(\"\\n\"))\n",
    "schemaDT = spark.createDataFrame(rdd_cursor, StringType(), True).cache()\n",
    "schemaDT.createOrReplaceTempView(\"rdd_cursor\")\n",
    "sqlDF1 = spark.sql(\"SELECT value, replace(replace(substr(value,instr(value,'from'),instr(value,';')),'from',''),';','') as library  FROM rdd_cursor where trim(value) like 'import%from%'  and replace(replace(substr(value,instr(value,'from'),instr(value,';')),'from',''),';','') not like ''     \" )\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "sqlDF2 = spark.sql(\"SELECT  value, replace(substr(value,instr(value,'from')),'from','') as library FROM sqlDF1 where library like '' \")\n",
    "DF =sqlDF1.union(sqlDF2)\n",
    "sqlDF1.createOrReplaceTempView(\"DF\")\n",
    "sqlDF3 = spark.sql(\"SELECT replace(replace(library,'../',''),'./','') as library  FROM DF  \")\n",
    "\n",
    "DF_final = sqlDF3\n",
    "\n",
    "DF_libcnt = DF_final.withColumn('library', regexp_replace('library', '\"', \"'\")).groupby(\"library\").count().orderBy(\"count\", ascending = False)\n",
    "\n",
    "DF_libcnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42873bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_libcnt.write.mode('append').json(\"s3://apollo-mission/apollo-simge-test/JAVA_Lib_cnt.JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_libcnt.write.mode('append').json(\"s3://apollo-mission/apollo-simge-test/Phyton_Lib_cnt.JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_libcnt.write.mode('append').json(\"s3://apollo-mission/apollo-simge-test/JAVASCRIPT_Lib_cnt.JSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
